- explore different reward values for different iterations of the enviornment
    - typically the rewards should be on the same order of magnitude
- right not we are plotting/benchmarking the learning curve of the agent, in other words 
    we are benchmarking the total score of that agent

- more things to benchmark/explore in the future:
    - episodes per second
    - asymptotic performance: the final "flat" curve of the total scores
    - sample efficientcy: the number of episodes it takes the agent to reach this flat line
    - training stability (how stable the scores are) -> need to do a deeper dive on this on how to actually implement it
    
- Connecting a NN to 2048 env:
    - build nn to predict Q(s, a)
    - starts off shit, random guesses basically
    - as we play games we accumulate data
    - this data can then be used to train nn for the game
    - in theory, our scores will then go up
    - inputs for NN:
        - current state (2d array of board, process as 4x4x1 image, lets us used cnns)
    - outputs:
        - predicted long term values of taking each one of the actions
            - output 1: predicted long term value of taking the action "up"
            - output 2: predicted long term value of taking the action "down", etc
    - how to create labels? use the bellman equation:
        - calculate the target of bellman's equation AS the label for the input of the nn
        - this way our nn will become more accurate at predicting the value of the targets of each action in the corresponding state
        -    
     
    - architecture of nn: make sure to have dropout, activation functions, will make sure to grow paramters and layers as we accumulate more data
        from more games being played
        - maybe have some augmentation functions for our data where we try to "diversify" our data, for example: lets say agent picks an invalid move
            5 times in a row with the same action, only need 1 of this, have script to clean our data  
        - starting off with 4x4x1 board, going to upsample so we can have more layers
            - then downsample a little bit, then feed into fully dense layers where we then 
                predict the value function (4 dimension final outputr\)
        
        - 4x4x1 -> 8x8x3 -> 8x8x16 -> 24x24x64 -> 16x16x32 -> 16x16x16 -> 8x8x16 -> 1024x1 -> 384x1 -> 128x1 -> 16x1 -> 4x1 


- the nn approach to the 2048 problem was too slow
- on policy monte carlo also might be too slow
    - going to reread the textbook and explore the off-policy behavior and target policy approach
        with the aim and hope that the off-policy approach converges faster and we actually begin to learn
- ALWAYS make sure that the enviornment is HIGHLY HIGHLY HIGHLY performance optimized
    as the majority of the "work" by the computer will be in the env.step()
    function of the rl problem

- sometimes there will be RL problems with near infinite states (lets say trillions /quadrillions in this case)
    - a HUGE optimization of these rl problems are to answer this question: "can we phrase and implement the intended rl solution where we minimize
        the amount of states"
            - if we CAN do this without effecting the accuracy of the value function then we will converge to the answer faster, we will use less compute, 
                less time to solve the rl Problem
            - if not, is there a possitive tradeoff between decreasing states and decreasing accuracy of value function
                - if states are too big then the answer will never converge, learning will take too long, it is better to learn lets say 50% of the accurate
                    value function then to never learn at all
            
numba for python notes:
    - dont mix types 
    - try to avoid using python lists with numba whenever you can
    - vectorize:
        - can input a list/array and then define what to do for each element in that array
    - try to not create dynamic arrays in numba and then return stuff,
        much better way is to pre-fetch / initialize a constant sized array with numpy, pass this numpy array into the numba function,
        and then just update the arrary in the numba function
    - try not to switch between python and numba alot in the same top-level python function 
        - try to configure it in a way where the numba is the top-level function and the python just allocates/sets stuff up
    - avoid creating new array entirely, create initial buffers that you manipulate
    - 
    