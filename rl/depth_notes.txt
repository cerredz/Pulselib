- explore different reward values for different iterations of the enviornment
    - typically the rewards should be on the same order of magnitude
- right not we are plotting/benchmarking the learning curve of the agent, in other words 
    we are benchmarking the total score of that agent

- more things to benchmark/explore in the future:
    - episodes per second
    - asymptotic performance: the final "flat" curve of the total scores
    - sample efficientcy: the number of episodes it takes the agent to reach this flat line
    - training stability (how stable the scores are) -> need to do a deeper dive on this on how to actually implement it
    
- Connecting a NN to 2048 env:
    - build nn to predict Q(s, a)
    - starts off shit, random guesses basically
    - as we play games we accumulate data
    - this data can then be used to train nn for the game
    - in theory, our scores will then go up
    - inputs for NN:
        - current state (2d array of board, process as 4x4x1 image, lets us used cnns)
    - outputs:
        - predicted long term values of taking each one of the actions
            - output 1: predicted long term value of taking the action "up"
            - output 2: predicted long term value of taking the action "down", etc
    - how to create labels? use the bellman equation:
        - calculate the target of bellman's equation AS the label for the input of the nn
        - this way our nn will become more accurate at predicting the value of the targets of each action in the corresponding state
        -    
     
    - architecture of nn: make sure to have dropout, activation functions, will make sure to grow paramters and layers as we accumulate more data
        from more games being played
        - maybe have some augmentation functions for our data where we try to "diversify" our data, for example: lets say agent picks an invalid move
            5 times in a row with the same action, only need 1 of this, have script to clean our data  
        - starting off with 4x4x1 board, going to upsample so we can have more layers
            - then downsample a little bit, then feed into fully dense layers where we then 
                predict the value function (4 dimension final outputr\)
        
        - 4x4x1 -> 8x8x3 -> 8x8x16 -> 24x24x64 -> 16x16x32 -> 16x16x16 -> 8x8x16 -> 1024x1 -> 384x1 -> 128x1 -> 16x1 -> 4x1 


- the nn approach to the 2048 problem was too slow
- on policy monte carlo also might be too slow
    - going to reread the textbook and explore the off-policy behavior and target policy approach
        with the aim and hope that the off-policy approach converges faster and we actually begin to learn
- ALWAYS make sure that the enviornment is HIGHLY HIGHLY HIGHLY performance optimized
    as the majority of the "work" by the computer will be in the env.step()
    function of the rl problem

- sometimes there will be RL problems with near infinite states (lets say trillions /quadrillions in this case)
    - a HUGE optimization of these rl problems are to answer this question: "can we phrase and implement the intended rl solution where we minimize
        the amount of states"
            - if we CAN do this without effecting the accuracy of the value function then we will converge to the answer faster, we will use less compute, 
                less time to solve the rl Problem
            - if not, is there a possitive tradeoff between decreasing states and decreasing accuracy of value function
                - if states are too big then the answer will never converge, learning will take too long, it is better to learn lets say 50% of the accurate
                    value function then to never learn at all
            
numba for python notes:
    - dont mix types 
    - try to avoid using python lists with numba whenever you can
    - vectorize:
        - can input a list/array and then define what to do for each element in that array
    - try to not create dynamic arrays in numba and then return stuff,
        much better way is to pre-fetch / initialize a constant sized array with numpy, pass this numpy array into the numba function,
        and then just update the arrary in the numba function
    - try not to switch between python and numba alot in the same top-level python function 
        - try to configure it in a way where the numba is the top-level function and the python just allocates/sets stuff up
    
Implementing a poker enviornment first ideas:
    - need 4 different actions, check, call, and raise, and fold
        - need way to put dollar amount inside of the raise action (how much to raise)
        - do we want to have env based off of dollar amount of blinds amount
            - prob go off of blind sizes, can have this as input to env
        - have blind amounts as input to env
        - do we want to play against 1 player? do we want to play against N players?
            - probably n players
            - will need to take into account how many players before and after agent takes action
        - how to calc reward?
            - how much money they win or lose after a full hand
                - too big numbers, need to normalize
                - dont want to wait until after hand to give out rewards
            - can we derive probability of opponents hand based on the cards on the board and our cards?
                - if we can do we give out rewards based on the action the agent takes
            -      

    - list of things that affect the way poker players play (not rl, just thinking about game):
    (these will be used for our states of the enviornment, will combine them into an array of values)    
    (note, the more we add here the more states that our rl env will have so the longer it will take to learn)
        - in the most optimistic case, we can just add everything here and just throw compute at the problem to solve the rl problem
            - however, we will most likely have to intuitively pick the "most important" poker factors from the below list so that our state-space does not explode
                - good idea is prob to start with very small state-space and only a few from the below list    
        - ALSO, VERY IMPORTANT, we will be assuming this poker env will be of cash games, not tournaments
            - how do we want to handle the logic of "refills"

        - cards that we have
        - cards that are on the board
        - "strength" of hand (two pair, trips, flush, etc)
        - number of players
        - position in the rotation of cards
        
        (list of things gotten from gemini)
        - effective stack size, stack-to-pot ratio, pot odds, m-ratio, 
        - sequence of actions
        - players actively in hand
        
        - how "aggressive a player is"
            - how active a player is, % of hands that they are playing
            - how much they raise pre-flop
            - 3-bet percentage
            - how often they fold
            -  


        - always use ints for blind amounts and not floats
        - simple action space:
            - fold, check, call, raise min, raise 33% pot, raise 75% pot, 150%, all in
                - this action space reduces the search space
                - slightly concerned though as it could be "limiting" the ability to place the optimal bet
                    - might add like 3-5 more "percentage" raises as solution to this

        - learning strat / how we are going to do rewards:
            - tricky question to tackle and find out which one is optimal
            
            
            - do we start off with agent playing against another random agent?
                - could win but only on the fact that the other agent makes a bad move
                - based on idea that both agents will continuously get better, each making the other better
                - 
            - do we start off agent playing against "experienced" agent
                - will start of with better learning feedback/data points
                - what if experienced agent is not optimal, our agent might learn to play against unoptimal opponents
                    or could get "hardstuck" up until a certain skill level
            
            - in a game of n players, we can have n agents each playing against each other and learning while playing
            -  
            
            - do we even need opponents/other agents when calculating reward?
            - we can try to play alot of games, feed data into nn, then continue playing games with that nn and updating it from there

            - how are we going to compute rewards if the pot ends at the end of each game?
                - in our simulations we know everyones hands, maybe we compute reward based on if they would win at the current time
                    - this could punish a player for winning a hand (losing at the current time)
                - compute reward based on the current probability of winning
                    - could punish agent for winning, but it learns to play off of favorable probabilities, which is essentially what we want
                        - dont want something that learns to win every time (going all in every time to try to win hand), just want something to win over the long term
                    - if we can compute probability at current time do we need rl?
                        - yes, even if same probability of winning it could still be in different state
                - could compute rewards for the longer that it is in the game
                    - like every step it survives is +1
                    - is very simple and could work, just fear that it will take too long to converge
                    - actually nvm, we also have to account that there are times where we DONT want to survive a long time (if we have 7-2 off)
                
                - i have the intuition that we should use the information of the other players cards to derive the reward value of a step
                    - although this agent will not have this information at run time, the intuition is that by deriving the reward from other players
                        this will allow us to gain a "better and more accurate" reward signal which will lead to the agent playing better
                            - we dont actually need to know the other players hands when making decisions, just need to know the reward of each action in each state
                - also need the reward to promote folding/ending their run in the game at the right times
                    - remember, sometimes the optimal and "winning" play is to lose the pot because the odds are not in our favor 
                
                - after consulting with gemini, there are multiple ways we can shape our reward, I think the best way to go 
                    is to shape the rewards based off of the probabilitie/have our agent learn the probabilities

                - more reward ideas:
                    - money won in pot, money won in last n pots, 
                
        - BREAKTHROUGH REWARD IDEA:
            - base the reward off of probability AND money made
            - base the reward off of the EXPECTED reward

        - final reward function:
            - w_money * delta(ev) + w_strat * 

        The Formula in One Paragraph
        The reward function calculates a score between -50 and +50 that balances financial outcomes with strategic correctness. It starts by computing a "Hybrid Money" value, which averages your theoretical Expected Value (Sklansky dollars) with your actual cash results to dampen the luck of the cards while keeping the agent grounded in reality. It then adds a "Strategy Score" that rewards you for acting in alignment with the mathâ€”specifically, folding when your chance of winning is lower than the pot odds offered, or betting when your equity exceeds the fair share. Finally, it squashes this combined raw score using a hyperbolic tangent function (scaled by K) to ensure that all rewards stay within a fixed range, preventing exploding gradients and stabilizing the neural network's learning process.
        
        - scaling stats:
            - assume 5,000,000 steps per second with parrallization (think i can get this higher)
            - assume 50 steps per game in the worse case (think this will be around 20 per game)
            - this will bring us to 100,000 games per second
                - 6,000,000 games per minute
                - 360,000,000 games per hour

        - state space: below will be the order in which we will represent things in our state
            - board -> 5 ints
            - our cards -> 2 ints
            - stage in game (preflop, flop, turn, river) -> 1 int, 0-3
            - position -> 1 int (from button)
            - pot size -> 1 float (normalized with blind size)
            - our money -> 1 int (normalized with blind size)
            - opponent info: 
                -> n player ints * 3
                    - 1 int for player status (0 for foled, 1 for active)
                    - 1 int for opponent stack size (big blinds)
                    - 1 int for opponent current round bet size
                    
            - bet -> 1 int (the amount we NEED to bet to stay in, could be 0 for just calling)

        - action space:
            - fold, call, check, bet 25%, bet 33%, bet 50%, bet 75%, bet 100%, 150%, 200%, 300%, all in

        - checklist: state space, action space, reward function, 

        - learning strat:
            - how are we going to get our agent to learn?
                - besides rl obv
            - 1 idea: self play against n other same agents (5 lets say)  
                - each one of them plays each other and each one of them learns
                - fear this this will just take way too long to get a meaningful well played poker agent
                    - will end up taking billions of games at least

            - "cheat" and have the agent play against other well known "poker bots"
                - intuition: starts off against semi-good opponent, this it will learn off of the start will be meaningful
                - once we consistently start performing well against these bots and 
                    consistently beating them, then we transition to self play against another one of our own bots
                
            - play against "heuristic pot odds bot"
                - simple and fast
                - dont have to compute decision of other agents
                - scale this to a few billion games, and then start self play
            
            - only con to self play besides scale issues:
                - self play might result in the agent learnign to play against a specific type of player
                - in poker, there are many different types of players and we need our agent to be well equipped for all types
                - essentially, when we "learn" how to beat the opponent in self-play then we are just training for that type of player
                    and the consequence is that we lose to all other types of players
                - 

        - revised learnign strat:
            - use "well known poker bots" for benchmarks/testing of how good our agent is
                - run like 50 episodes every 1 mil episodes against these bots
            
            - **start off with "heuristic" or other lower skilled bots, run this for a few million episodes so that our agent
                gets a good baseline**
                - converged onto this idea, will begin with it and will still think about how to scale self-play
                    - scaling the self play and doing it correctly is the most important question, will implement everything else and then fall back to this question
            
            - explore nash equilibrium
            - maybe combine scaled-self play with "varied stregists" (play against different types of bots)


    - environment notes:
        - now have to start implementing the environment
        - going to have n players in the env
            - user simply passes in N
        - going to define an abstract Player class
            - going to have an action function 
                - this is where we can differentiate the actions of each bot
                - heuristic bot, random bot, or our bot
        - going to have a current idx with the player that turn it is on
            - the state and step function rely on this idx
        
        - apart from above, will implement game logic in most efficient and fast way possible 


    - more notes:
        - explore more complex reward function with more poker data points
        - explore "biased" reward functions for different agent for our main agent to play against
            - "biased" folding agent,"biased" raising agent, etc