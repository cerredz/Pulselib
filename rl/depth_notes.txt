- explore different reward values for different iterations of the enviornment
    - typically the rewards should be on the same order of magnitude
- right not we are plotting/benchmarking the learning curve of the agent, in other words 
    we are benchmarking the total score of that agent

- more things to benchmark/explore in the future:
    - episodes per second
    - asymptotic performance: the final "flat" curve of the total scores
    - sample efficientcy: the number of episodes it takes the agent to reach this flat line
    - training stability (how stable the scores are) -> need to do a deeper dive on this on how to actually implement it
    
- Connecting a NN to 2048 env:
    - build nn to predict Q(s, a)
    - starts off shit, random guesses basically
    - as we play games we accumulate data
    - this data can then be used to train nn for the game
    - in theory, our scores will then go up
    - inputs for NN:
        - current state (2d array of board, process as 4x4x1 image, lets us used cnns)
    - outputs:
        - predicted long term values of taking each one of the actions
            - output 1: predicted long term value of taking the action "up"
            - output 2: predicted long term value of taking the action "down", etc
    - how to create labels? use the bellman equation:
        - calculate the target of bellman's equation AS the label for the input of the nn
        - this way our nn will become more accurate at predicting the value of the targets of each action in the corresponding state
        -    
     
    - architecture of nn: make sure to have dropout, activation functions, will make sure to grow paramters and layers as we accumulate more data
        from more games being played
        - maybe have some augmentation functions for our data where we try to "diversify" our data, for example: lets say agent picks an invalid move
            5 times in a row with the same action, only need 1 of this, have script to clean our data  
        - starting off with 4x4x1 board, going to upsample so we can have more layers
            - then downsample a little bit, then feed into fully dense layers where we then 
                predict the value function (4 dimension final outputr\)
        
        - 4x4x1 -> 8x8x3 -> 8x8x16 -> 24x24x64 -> 16x16x32 -> 16x16x16 -> 8x8x16 -> 1024x1 -> 384x1 -> 128x1 -> 16x1 -> 4x1 


- the nn approach to the 2048 problem was too slow
- on policy monte carlo also might be too slow
    - going to reread the textbook and explore the off-policy behavior and target policy approach
        with the aim and hope that the off-policy approach converges faster and we actually begin to learn
- ALWAYS make sure that the enviornment is HIGHLY HIGHLY HIGHLY performance optimized
    as the majority of the "work" by the computer will be in the env.step()
    function of the rl problem

- sometimes there will be RL problems with near infinite states (lets say trillions /quadrillions in this case)
    - a HUGE optimization of these rl problems are to answer this question: "can we phrase and implement the intended rl solution where we minimize
        the amount of states"
            - if we CAN do this without effecting the accuracy of the value function then we will converge to the answer faster, we will use less compute, 
                less time to solve the rl Problem
            - if not, is there a possitive tradeoff between decreasing states and decreasing accuracy of value function
                - if states are too big then the answer will never converge, learning will take too long, it is better to learn lets say 50% of the accurate
                    value function then to never learn at all
            
numba for python notes:
    - dont mix types 
    - try to avoid using python lists with numba whenever you can
    - vectorize:
        - can input a list/array and then define what to do for each element in that array
    - try to not create dynamic arrays in numba and then return stuff,
        much better way is to pre-fetch / initialize a constant sized array with numpy, pass this numpy array into the numba function,
        and then just update the arrary in the numba function
    - try not to switch between python and numba alot in the same top-level python function 
        - try to configure it in a way where the numba is the top-level function and the python just allocates/sets stuff up
    - avoid creating new array entirely, create initial buffers that you manipulate
    
Implementing a poker enviornment first ideas:
    - need 4 different actions, check, call, and raise, and fold
        - need way to put dollar amount inside of the raise action (how much to raise)
        - do we want to have env based off of dollar amount of blinds amount
            - prob go off of blind sizes, can have this as input to env
        - have blind amounts as input to env
        - do we want to play against 1 player? do we want to play against N players?
            - probably n players
            - will need to take into account how many players before and after agent takes action
        - how to calc reward?
            - how much money they win or lose after a full hand
                - too big numbers, need to normalize
                - dont want to wait until after hand to give out rewards
            - can we derive probability of opponents hand based on the cards on the board and our cards?
                - if we can do we give out rewards based on the action the agent takes
            -      

    - list of things that affect the way poker players play (not rl, just thinking about game):
    (these will be used for our states of the enviornment, will combine them into an array of values)    
    (note, the more we add here the more states that our rl env will have so the longer it will take to learn)
        - in the most optimistic case, we can just add everything here and just throw compute at the problem to solve the rl problem
            - however, we will most likely have to intuitively pick the "most important" poker factors from the below list so that our state-space does not explode
                - good idea is prob to start with very small state-space and only a few from the below list    
        - ALSO, VERY IMPORTANT, we will be assuming this poker env will be of cash games, not tournaments
            - how do we want to handle the logic of "refills"

        - cards that we have
        - cards that are on the board
        - "strength" of hand (two pair, trips, flush, etc)
        - number of players
        - position in the rotation of cards
        
        (list of things gotten from gemini)
        - effective stack size, stack-to-pot ratio, pot odds, m-ratio, 
        - sequence of actions
        - players actively in hand
        
        - how "aggressive a player is"
            - how active a player is, % of hands that they are playing
            - how much they raise pre-flop
            - 3-bet percentage
            - how often they fold
            -  


        - always use ints for blind amounts and not floats
        - simple action space:
            - fold, check, call, raise min, raise 33% pot, raise 75% pot, 150%, all in
                - this action space reduces the search space
                - slightly concerned though as it could be "limiting" the ability to place the optimal bet
                    - might add like 3-5 more "percentage" raises as solution to this

        - learning strat / how we are going to do rewards:
            - tricky question to tackle and find out which one is optimal
            
            
            - do we start off with agent playing against another random agent?
                - could win but only on the fact that the other agent makes a bad move
                - based on idea that both agents will continuously get better, each making the other better
                - 
            - do we start off agent playing against "experienced" agent
                - will start of with better learning feedback/data points
                - what if experienced agent is not optimal, our agent might learn to play against unoptimal opponents
                    or could get "hardstuck" up until a certain skill level
            
            - in a game of n players, we can have n agents each playing against each other and learning while playing
            -  
            
            - do we even need opponents/other agents when calculating reward?
            - we can try to play alot of games, feed data into nn, then continue playing games with that nn and updating it from there

            - how are we going to compute rewards if the pot ends at the end of each game?
                - in our simulations we know everyones hands, maybe we compute reward based on if they would win at the current time
                    - this could punish a player for winning a hand (losing at the current time)
                - compute reward based on the current probability of winning
                    - could punish agent for winning, but it learns to play off of favorable probabilities, which is essentially what we want
                        - dont want something that learns to win every time (going all in every time to try to win hand), just want something to win over the long term
                    - if we can compute probability at current time do we need rl?
                        - yes, even if same probability of winning it could still be in different state
                - could compute rewards for the longer that it is in the game
                    - like every step it survives is +1
                    - is very simple and could work, just fear that it will take too long to converge
                    - actually nvm, we also have to account that there are times where we DONT want to survive a long time (if we have 7-2 off)
                
                - i have the intuition that we should use the information of the other players cards to derive the reward value of a step
                    - although this agent will not have this information at run time, the intuition is that by deriving the reward from other players
                        this will allow us to gain a "better and more accurate" reward signal which will lead to the agent playing better
                            - we dont actually need to know the other players hands when making decisions, just need to know the reward of each action in each state
                - also need the reward to promote folding/ending their run in the game at the right times
                    - remember, sometimes the optimal and "winning" play is to lose the pot because the odds are not in our favor 
                
                - after consulting with gemini, there are multiple ways we can shape our reward, I think the best way to go 
                    is to shape the rewards based off of the probabilitie/have our agent learn the probabilities

                - more reward ideas:
                    - money won in pot, money won in last n pots, 
                
        - BREAKTHROUGH REWARD IDEA:
            - base the reward off of probability AND money made
            - base the reward off of the EXPECTED reward

        - final reward function:
            - w_money * delta(ev) + w_strat * 

        The Formula in One Paragraph
        The reward function calculates a score between -50 and +50 that balances financial outcomes with strategic correctness. It starts by computing a "Hybrid Money" value, which averages your theoretical Expected Value (Sklansky dollars) with your actual cash results to dampen the luck of the cards while keeping the agent grounded in reality. It then adds a "Strategy Score" that rewards you for acting in alignment with the math—specifically, folding when your chance of winning is lower than the pot odds offered, or betting when your equity exceeds the fair share. Finally, it squashes this combined raw score using a hyperbolic tangent function (scaled by K) to ensure that all rewards stay within a fixed range, preventing exploding gradients and stabilizing the neural network's learning process.
        
        - scaling stats:
            - assume 5,000,000 steps per second with parrallization (think i can get this higher)
            - assume 50 steps per game in the worse case (think this will be around 20 per game)
            - this will bring us to 100,000 games per second
                - 6,000,000 games per minute
                - 360,000,000 games per hour

        - state space: below will be the order in which we will represent things in our state
            - board -> 5 ints
            - our cards -> 2 ints
            - stage in game (preflop, flop, turn, river) -> 1 int, 0-3
            - position -> 1 int (from button)
            - pot size -> 1 int (normalized with blind size)
            - our money -> 1 int (normalized with blind size)
            - opponent info: 
                -> n player ints * 3
                    - 1 int for player status (0 for foled, 1 for active)
                    - 1 int for opponent stack size (big blinds)
                    - 1 int for opponent current round bet size
                    
            - bet -> 1 int (the amount we NEED to bet to stay in, could be 0 for just calling)

        - action space:
            - fold, call, check, bet 25%, bet 33%, bet 50%, bet 75%, bet 100%, 150%, 200%, 300%, all in

        - checklist: state space, action space, reward function, 

        - learning strat:
            - how are we going to get our agent to learn?
                - besides rl obv
            - 1 idea: self play against n other same agents (5 lets say)  
                - each one of them plays each other and each one of them learns
                - fear this this will just take way too long to get a meaningful well played poker agent
                    - will end up taking billions of games at least

            - "cheat" and have the agent play against other well known "poker bots"
                - intuition: starts off against semi-good opponent, this it will learn off of the start will be meaningful
                - once we consistently start performing well against these bots and 
                    consistently beating them, then we transition to self play against another one of our own bots
                
            - play against "heuristic pot odds bot"
                - simple and fast
                - dont have to compute decision of other agents
                - scale this to a few billion games, and then start self play
            
            - only con to self play besides scale issues:
                - self play might result in the agent learnign to play against a specific type of player
                - in poker, there are many different types of players and we need our agent to be well equipped for all types
                - essentially, when we "learn" how to beat the opponent in self-play then we are just training for that type of player
                    and the consequence is that we lose to all other types of players
                - 

        - revised learnign strat:
            - use "well known poker bots" for benchmarks/testing of how good our agent is
                - run like 50 episodes every 1 mil episodes against these bots
            
            - **start off with "heuristic" or other lower skilled bots, run this for a few million episodes so that our agent
                gets a good baseline**
                - converged onto this idea, will begin with it and will still think about how to scale self-play
                    - scaling the self play and doing it correctly is the most important question, will implement everything else and then fall back to this question
            
            - explore nash equilibrium
            - maybe combine scaled-self play with "varied stregists" (play against different types of bots)


    - environment notes:
        - now have to start implementing the environment
        - going to have n players in the env
            - user simply passes in N
        - going to define an abstract Player class
            - going to have an action function 
                - this is where we can differentiate the actions of each bot
                - heuristic bot, random bot, or our bot
        - going to have a current idx with the player that turn it is on
            - the state and step function rely on this idx
        
        - apart from above, will implement game logic in most efficient and fast way possible 

    - more notes:
        - explore more complex reward function with more poker data points
        - explore "biased" reward functions for different agent for our main agent to play against
            - "biased" folding agent,"biased" raising agent, etc

        next steps:
            - rly dive into the poker env and players, see why each game is lasting 100,000 + steps
                - think it has to do with either heuristic or q learning bot repeatedly calling 
                    same "invalid" actions that is causing the game to stall
                    - will add runtime errors everywhere just for manual debug, then will   
                        apply fixes

                - each game should realistically only be at most 50-75 steps 
        - experiement with moving the raise_amounts tensor outside of the actions function and into the 
            reset function

    - fuck it, switching to neural nets for value function approximation
        - trying to grow a q-table on gpu is going to be very hard, alot of time, and quite honestly
            prob going to be way too much memory anyways in the end
        - going to built nn to approxiate q-values
        - going to start off nn small, might built different versions of it later
            - going to save instance of different nns and maybe play them later on
        - 

    - pytorch notes:
        - 

    - everything is now hooked up and the main bottleneck is goind to be training time, however in the meantime, what are a list of decisions that we can implement that is going to improve our model:
        - 1) need to actually implement equity calculation, right now our model is basically only learning the "pot odds" part of the reward function
        - 2) make the qlearning network deeper and larger, right now pretty small, wanted to start off like this, but should prob make it bigger
        - 3) implement "harder" players for out newtwork to play against
            - right now, our network is playing against week oppponents with the intuition of just learning the basics of poker
                - simple heuristic and random agent
            - implement harder heuristics, also implement hueristic agents with a biased towards different styles of play
        - 4) start to maybe incorperate self-play once the model gets good enough
        - 5) incorperate interface where we can actually test and play the model (manual debug)
        - 6) when playing, switch the positions of the players each time we end an episodes
             (if we keep lets say random agent in seat 3 across 5,000 runs, then our agent will begin to learn and think that seat 3 will always have a random agent)
        - 7) another thing that we have to account for is training for different sizes games, not all games are going to be 6 players:
            - thing I am going to input a "max_players" attribute into the environment, and however many players there are we will fill the state-space of opponent info
                of zeros based on how many players there are off of the max player number
                - will increase state space a little bit, but is going to enable us to train against multiple lengthed opponents
                - have to cut/append opponent length while training
                    - add different players into each slot as well
        
        - explore pots bug, explore resolve fold winners, explore active players


    - equity caluclation solved (for now using 5-6-7 card evaluators):
        - preflop and river we use precomputed tables
        - turn we use a 1-step lookahead table:
            - there are 40-45 cards left in the deck, just simulate each one of these cards on the river and calculate the 
                equity from these precomputed tables there
        - post flop, do same as above (this will not be as accurate since we cannot see the river)
    
    - i noticed that we are not perfectly calculating the equity at each stage, originally, I had thought that this
        would be a simple alg at each stage of the game with 100% accuracy
            - it is actually very computational and even then not 100% (just simulates a large amount of games)
         - this could end up fucking us, as the equity calculation is apart of the reward function, it needs to be accurate
            - note: the equity calc is extremely accurate for the current player (doesnt take into account other players)
                this in theory could still take us VERY FAR and have a VERY ADVANCED BOT
        
        - ideally, we would want to calculate equities with 100% accuracy, blazingly fast, and using 
            ALL players card info (not just 100% accuracy with current players hand)
            - unfortunately, i just dont think that this is possible with the amount of compute/memory we have
                (think it would be in the petabytes to create a lookup table for this)
        
        - this might lead us later on if we notice the bot is not improving to reshape the reward function
        - potential reward function ideas:
            - profit in the last N games (10, 25, 100, 1000)

    - how to handle reward calculation for folded players?
        - nevermind, the reward for folded players will be 0 and the terminated states for these players will be 0
            - follows q-learning where reward(terminated) states=0

    - right now we are using the standard 5-6-7 handrank.dat file for equitiy calculation   
        - problem: at lets say turn, it provides the hand strength, not true equity
            - for true equity we must simulate all possible river cards and average hand strength among each of them
            - fear this will take too long (both in compute and implementing), so for right now we 
                will use the standard 5-6-7 hand strength for equity calculation

    - when training, our reward is going up but our chip count is steadily going down in the graph
        - this might not actually be because the bot is losing money (it could be it is continued to go down with more training)
            - but what could also be happening is how we handle the stack logic after each reset
        - if the agent gets to a point of over the maximum number of big blinds, the stack is reset to the original starting blinds number
            - this would obviously be a HUGE loss of chips, which can affect the profit count MASSIVELY across millions of games
        - it is fine for now, right now it is only important that our reward is going up (which it is), and will will investing solutions to the stack sizes later

            - could reset stack after every episode:
                - pros: easier to compute profit after each episode, profit number is realistically
                - cons: nn will only learn/have state spaces around the starting bbs, which is not ideal for real-world cases (stack size ranges in real poker games)

            - prob just going to return indices that have reached over the max bbs, then compute profit based on these seats (dont take into account negatives)

- new state space: 

0:5      → board (5)
5:7      → own hand (2)
7:8      → stage
8:9      → position relative to button
9:10     → pot size
10:11    → to-call (highest - my_bet_this_round)
11:12    → my stack
12:13    → my status (folded/active)
13:18    → opponent stacks (5)
18:23    → opponent active masks (5)
23:28    → opponent bets this round (5)

- look into reward calcuation with stack changes (think we could be unintentionally decentiving ALL bets via stack changes)
- explore and look into the build_agents bottleneck
    - try to first optimize entire function
    - try to optimize the specific players (i think we have alot of repeat computations)

- do a deeper dive on optimizing "or" and "and" boolean masks on tensors
    - thing we can do clever trick with math to speedup performance

- only run the calculate_equities function when the STAGE changes (computed it repeatedly) at each step
    equities dont change until the stages change
    - basically doing the same thing 3 times in this function, try to batch it into 1

- reward function right now:
    - stakes into account equity, size of pot, and pot odds,

    - what we still need the reward function to take into account:
        - position, our stack size, numer of opponents remaining

BLACK JACK ENVIRONMENT:
    - in order to test the stability of my deep q-agent, i will impliment a very simple blackjack environment
        with a very simple deep q-learning neural network agent to make sure that we get the fundamentals right

    - add replay buffer support for parallized tensor environments
        - 

    - what i am going to do tomorrow:
        - replay buffer full implementation
        - double dqn full implementation
        - dqn + no replay buffer, dqn + replay buffer, double dqn no replay, double dq replay benchmarks

        - refactor some code, make the training scripts easier to setup
            - make storing results, plots, runs, etc, easier

- next agents that I want to implement for RL algs:
    - deep sarsa, 
    - deuling DQN
        - is basically the same as bellman, but splits the q value prediction into 2
            - calculates value of S regardles of action, how valuable is being in this state
            - calculates advantage(s, a) -> how much better is taking the predicted action than the average action value seen in this state

    - actor critic
    - recurrent DQN

    - DDPG:
        - model-free off policy rl alg designed to handle continuous action spaces
        - learns determinstic policy directly
        - actor:
            - maps the state to an action
            - outputs an action and not a probability distribution
            - 
        - critic:
            - estimates the q-value pairs

        - sample from the replay buffer every train_step
        - actor, critic, target actor, target critic
        - soft updates
        - exploration via generating noise for the action selection
        - critic gets better at accurately predicting q-values, and then backpropagates this signal to the actor network so that it can select better actions
            - replay buffer keeps this process stable
        - actor:
            input: states
            output: action 
        - weights of network are initialized very small to have predictions near zero_grad
            - gradient clipping for the critic
        -  