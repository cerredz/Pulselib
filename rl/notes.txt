- state, action, rewards, vectors, weights
- rl = learning through interactions
- Capital=random variables, Lowercase=Value of random variable
- 

CHAPTER UNO (1):

- connecting and interacting with your enviornment is a major source of information
    - used int the real world to teach humana things

- we are aware of how our enviornment responds to the things that we do
- rl is focused on "goal-directed" learning from interactions
- want to maximize the reward signal
- learner is not told which actions to take, but rather given the reward
    of the actions and it is up to them to try them out
- trial-and-error search and delayed reward
- rl is NOT supervised learning and NOT unsupervised learning
- agent has to use what it already knows AND explore actions that it has not yet taken
- subproblems of rl enviornments should play clear roles and actually be steps
    towards reaching the central goal of the agent
- modern artificial intelligence is currently looking for methods of 
search, general principles of continuous learning, and decision making
- rl considered many subproblems, we do not tell them explicity what subproblems to 
    try, the agents use their own intuition to explore which subproblems to solve
- rl addresses the scenario of now knowing exactly what to do and 
    real-time action selection
- rl could be a piece of a much larger system, an rl env of
    a body part could feed its result to the rl env of the entire body
- rl has been looks at by many disciplines, not just AI 
- rl most accurately reflects how humans and animals act, and is very apparent in neuroscience and physh
- 

1.2) examples:

- chess moves
- adjust parameters of a operation in real time while taking into account multiple different tradeoffs
- animals in real life
- all the steps in eating breakfast in real life    
    - every decision bounded by a goal
    - each food, must first look for fork, feedback of eyes makes picking up fork possible, then use hand to guide food to eat, done eating, get up, etc 
- rl involves interaction from a decision-making agent, that agents seeks to accomplish a goal despite there being uncertainty in the enviornment
- the effect of actions may not always be able to be 
    fully predicted, so the agent must monitor the response of its env
    - percentage towards goal is determined by what the agent knows in its current state

- interacting  with the enviornment is essentially for IMPROVING the skills of the agent in that task,
    the more they interact with it the better they will become in the future

1.3) elements of rl 

- policy, reward signal, value function, and maybe a model
- policy:
    - defines the way an agent behaves at a given time
    - policies can be stochastic, we can specify probabilities for each action
    - mapping from the enviornment in the current state to the actions it can perform

- reward signal:
    - the goal of the rl problem
    - each action will in return send a reward value to the agent
    - the agent's sole focus is to MAXIMIZE this reward signal over a long period of time
    - 

value function:
    - takes into account the rewards in the LONG TERM
    - takes into account the next states likely to occur next, and calculates the reward for these states combined
    - total amount agent can expect in the future

- immediate reward signal could be low, but the value function could be very high
- rewards are viewed as primary and values are viewed as secondary
    - with no rewards there would be no values
- however, the final decisions that we take are based on the value of value functions
- the most important part of RL is the efficient estimation of values
- model:
    - models are used for planning, we want to predict future states and future rewards before we actually experience them
    - 
- rl env can have both low-level trial and error learning and high-level planning learning

1.4) limitations and scope:

- state: whatever information is available to the agent at a given timeframe
    - what the agent knows at a given time period
- book focused on not creating the state, but what actions the agent should take GIVEN the state
- estimating value functions is NOT required for solving rl problems

1.5) extended example 
- states: table of states, one for each possible state of the game
- value function for state A is "better" than value function for state B if our probability
    of winning in state A is higher
    - if our state has 3 x's, then our prob of winning is 1
    - if state has 3 o's, then prob of winning is 0
    - if neither, prob of winning is .5

- we play many games, most of the time we select states GREEDILY (the states that result in a higher value function)
- occasionally though, we select states "exploratory", the states that dont have a higher value function
    for the sole purpose of exploring different moves
- when we move, we update the previous state's probability to be more accurate to the true probability, since know we can see father ahead
- temporal difference learning: learning is done based on two different states at 2 successive times

- evolutionary methods BAD: if a player wins, all of the states and NON explored states are deemed good as the player won, but this may not be the calculates
- with value function, each state along the way can be used for the agent to learner
    - value function=each state is evaluated
- rl enforces the agent to look ahead and not just determine the immediate rewards of its actions
- reward for rl does not have to the at the end of an episode, it can be before it embeddings
- you can also apply rl to continuous time problems, not just ones with discrete reward signals

- you can setup rl with even infinately many states, the agent will only explore a small
    subset of these states but the final result can still "learn" how to solve the problem the best it can
    - this is a use case for supervised learning combined with rl, need to generalize on 
        our past experiences to make the best possible ones in the future

- rl can be used with or without the use of models to make decisions
- model free systems: enviornments do not have any influent on their actions
- an rl system can be a heirarchical rl system, where each "action" is a seperate 
    rl system


1.6) Summary

- formal framework of rl: states, actions, rewards, 
- value functions for rl are different from evolutionary methods of rl


1.7) History of RL

- one thread of rl was based on trial-and-error in animal learning
- second thread is value function
- "optimal control" threa:
    - first "rl" that solved a problem was known as dynammic programming: the state of the system
        determined the value that it returned
    - solved stochastic optimal control problem
    - the computational requirements grew with the amount of states there where
    - was not obvious how dp can be used to see thinks FORWARD in time since it always relies on 
        information in the past
- optimal control, dp, and rl are very closely intertwined
- "trial and error" thread:
    - "law of effect" -> describes the effect of reinforcing events on the tendency to select actions
    - reinforcement is the strengthening of a behavior based on an animal recieving stimuli
        - or could also be weakening
    - reinforcment produced changes in behavior AFTER the reinforce is removed
    - alan turing "pleasure-pain" system, when pain stimulus occurs all entries are canelled,
        when pleasure stimulus occurs then they are all made permanent
    - first employed and tested with animals
    - rl transitioned in supervised learning, many people thought they were studying rl but were studying   
        nns and supervised learning (generalizable patterns = sl)
    - "klopf" is a g that came along and basically said: hey we should more heavily reply on teh 
        enviornment to learn, rl and supervised learning are different
    - 
- "temperal difference" learning/thread:
    - secondary reinforces are paired with primary reinforces, and whatever behavior the primary
        reinforce triggers is also inherited down to the secondary reinforcer

questions:
    1.3) it would learn to play worst because it never explores other moves. These explored other moves
        that it does not experience could lead to a greater LONG-TERM reward that it will never get to.
    1.4) two sets of probabilities: 
        - we learn the exploratory and non-exploratory set
        - it would be better to learn from exploratory set because we factor in more total
            possible decision paths

    1.5)
        -  play a shit ton of games
        - dont just have all possible states of the tic-tac-toe game, maybe have states whose reward
            is based off of the previous position/s of the game 
        - play against all different types of opponents
        - play multiple games at once, reward is how many games you win in that batch

Chapter 2) multi-arm bandit (page 45)

- rl used the evaluation of actions instead of the instruction of actions to learn
- the feedback of the evaluation is what is necessary for learning in rl, FEEDBACK determines how 
    good the action was
- instruction feedback does not take into account the action itself, just the correct action to take
- evaluation feedback can be combined with instruction feedback

bandit problem:
- repeatedly faced with n different actions, each with a reward, goal is to get the highest reward 
    over certain number of steps
- we do not know the value of the actions beforehand
- select greedily: select action with the highest current estimated reward
- select exploringly: do not select the current highest valued action, with the intuition to explore
    other paths of actions in favor of long-term value benefits
- if you have many steps remaining, it could be better to actually explore the ungreedy actions because 
    we dont know if they will produce a higher value or not and we have many steps to go
- whether it is better to explore or to exploit depends on the estimation of values, uncertainty of 
    those values, and how many steps are left in the rl algorithms
- balancing exploration and exploitation is very big challenge in rl, below are some methods to show
    you the intuitions behind them

action-value methods:
    - each estimate is a simple average of the sample of relevant rewards
    - greedy action: A=argmax(Q(t)(a)) -> a maximizes all q at given step T
    - if we have alot players, and a certain amount of actions, then all actions will theoretically
        be sampled infinately and the estimated value to converge to the ground truth value
    - each action's value is basically the average of the reward of that action experienced
         in the past

    - e-greedy parameter: how often we are going to "explore", and not select the best valued action at the time
        - in armed testbed example, the higher the e-greedy parameter and the more the model "explored", the better it actually got
    - another thing you can do is to decrease e over time to try to get the best of both worlds (exploring and exploiting)
    - with noiser rewards, and a bigger variance in reward values, it could take more exploring to actually find the optimal action
    - rl required balance between exploration and exploitation 
    - if reward variance is 0 or super low then greedy action might win over exploring, even in deterministic low variance action values
        we should still explore because the action values could change over time across diffferent steps


2.4) Incremental Implementation:
    - explored how we can compute the sampled average of explored rewards in constant time and constant memory
    - new optimal update rule:
        - new estimate = old estimate + step_size(target-old estimate)
    - the step size parameter will change from time step to time step
    - have arrays Q and N where these arrays are constant in memory (they still use memory, but do not grow), and then we use them to compute 
        the constant time action values

2.5) Tracking Non-Stationary Problem:
    - methods discussed prior are appropriate for stationary bandit problems (rewards do not change over time)
    - often time rl problerms will be nonstationary and rewards will change over time
    - step size parameter is essentially used to give more weight to more recent rewards and less weight to long "in the past" rewards
        - the weight will decay exponentially
    - there are two conditions in stochastic approximation theory that must be satisfied to ensure that the action values have converged
        to their true values (way of ensuring that we have ran the rl alg enough times with enough selected actions to ensure that they represent
        their true value)
        - if estimated continue to vary based on most recent rewards, then the alg has not converged yet
    - the step size parameter enables constant improvements and smoothing in errors OVER time

2.6) Optimistic Initial values
    - previous methods all assumed an intial estimate
        - this means that the final outcomes were BIASED toward these initial estimates
    - multiple benefitial AND unbefitial reasons for initial estimates:
        - can give the alg some PRIOR KNOWLEDGE of the ground truth values of rewards
        - becomes a setting that needs to be hand-tweaked by a user
    - initial action values far from their ground truth values can actually be used as a strategy for exploration
        - called optimistic initial values approach:
            - good for stationary problems, is basically a TEMPORARY trick for exploration
    - if rewards change, nothing can be done to "activate" this exploration again, the beginning only happens once, and we should not place 
        all of our focus on optizing only the beginning of reward values

2.7) Upper confidence bound action selection:
    - when selecting non-greedy actions and exploring, wouldnt it be best to select the ones that are closest to the maximal action value 
        and also how confident we are in the actual estimation?
    - can do this with the upper confidence bound formula (page 57)
    - the square root term is how uncertain we are in the estimate of a's value, and c is the confidence level of this uncertainty
    - when we increment the number of times we select action A, the uncertainty of this estimate decreases (bottom term gets bigger)
        - each time we dont select action A then the uncertainty increases
    - the most frequently selected actions will have less uncertainty than the least frequently selected actions
    - the term c is used for the exploration bonus

2.8) Gradient Bandit alg

    - we can also assign a preference to an action seperate of its value, the higher the preference the more often the action is taken
        - this preference has no correlation to the reward of the action
    - 

2.9) Associative Search:
    - so far we have looked at actions that have not changed
    - there are problems where the actions change depending on the current task
    - sometimes in a singular rl problem there could be different tasks and for each different task the value of the same action can be different,
        we need a way to account for this

2.10) Summary
    - e-greedy method, UCB method, and Gradient bandit method, all discuss different ways of balancing and trying to optimize exploration and 
        exploitation
    - only the start of what we are able to do in terms of balancing exploration-exploitation, will explore much more advanced methods 
        later on in the book


CHAPTER 3) SEEMS LIKE A VERY IMPORTANT CHAPTER

- MDP -> instead of just estimating the value of an action (Q(A)), we will know begin to estimate the value of an action given the state Q(S, A)
- MDPS are mathematically format for rl, and this chapter will explore the applications of MDP and some of the trade-offs

- 3.1) agent environemnt interface

- agent -> the thing that learns and makes decisions
- enviornment -> the thing that the agent interacts with , carries out the logic of the agent's actions, sets up rewards and returns them to the agent with the new states
    - agent tries to maximize these rewards
-  basic process of time steps of the agent-environemnt interation: state 0, action 0, reward 0 -> state 1, action 1, reward 1, etc
- finite MDP, finite set of states and actions
    - because states and actions are finite, rewards and future states become a probability distribution
- in an MDP, the future state and rewards is based only on the direct preceding state and action, this is a RESTRICTION, we want to determine our decisions not only on direct 
    preceding action and state, but multiple steps back in time (ideally)
- Markov property: state has information about all prior interactions with the environemnt
    - formula for probability of S(T) and R(T) based on preceding state and action
    - formula for probability of random state given preceding state and action
    - formula for probability of reward given state and action

- actions are very arbitrary, can be low-level actions of high-level actions
    - could even have an action for "thinking" about the next action to take
- states work the same way, they can be low level, high level, subjective, etc
- actions: any decisions we want to learn to make, states: anything we know that could be useful to the decisions we want to make

- in the real world sense, moving a human's arm or leg are not actually the actions, but they are apart of the environemnt, not everything that is physical is an "agent" or action
    - anything that cannot be changed by the agent is generally considered part of the environemnt (human cannot change how many arms they have, or the length of their arms)
- in some cases the agent could know EVERYTHING about the environement perfectly, but still struggle to solve the problem
- agent-environement boundary is typically decided once we determine the actual task we are trying to solve
    - high-level agent could interact with an eviornment and create the states to then be inputted into a low-level agent to actually do the actions of the high level agent    

- MDP is widely used, it might not be the most optimal for decision-learning problems but many people use it and is the most popular framework for these problems in rl
- the states and action vary from task to task, and are more of an ART than science
examples of states and actions:
    - both states and actions could actually be VECTORS, lists of numbers, rewards however are always singular numbers
    - 

- the MDP framework is not adequate and usable for systems where the agent does not know the full state, it would not satisfy the MARKOV PROPERTY:
    ex: poker ai player, the agent would not know the full state (other players cards)
        - fortnite ai player: the game and the entire meta could change and there could be a whole new set of states that are created

- robot example:
    - states are the battery levels of the robot
    - the actions are different in each state, when the robot has a high battery we never want it to charge, so we remove this option entirely from the state
    - the robot gets small rewards when it picks up garbage, but LARGE rewards when it runs out of battery (we never want it to run out of battery)

3.2) GOALS AND rewards

- reward is a simple number over a time step, the goal of the agent is to maximize the TOTAL reward across all time steps
- reward signals are the formalization of what the agents 'goals' are
- WE must setup the rewards in a way that makes the agent achieve the actual goals and tasks we want it to achieve (rewards and how we set them up are very important)
    - rewards signal is basically the way of communicating to the agent

3.3) Returns and episodes:
    - The expected return is the sum of the rewards across all time steps in an agent-env interaction 
    - each episode ends in a terminal state, then after this a new starting state is determines for the next episode
    - the time of termination is usually a random variable that differs from episode to episode, each episode is independent of one another
    - somtimes the agent-environement problems has "continual tasks" in which the tasks may never end, in this case a problem arises: the total reward  
        of this task will always approach infinity
        - introduce discount rate: future rewards are only worth a fraction of what they would be worth at this current time step
            - intuition for this: remove the ability for the total reward of a task to approach infinity
    - lets say that the reward for something is the time in which it takes that task to FAIL, in the case where the agent learns alot and gets really good at the task
        the task could theoretically run forever in the current episode, and this is where we would have to use discounting

3.5) Policies and Value Functions:
    - almost all rl algs have way of estimating the reward value of being in current states or how good it is to perform an action where "how good" denotes the estimation
        of future reward values
    - value function are defined with particular ways of acting, called policies
    - policy is a mapping of states to a probability of selecting each possible action
    - value function and policy can be estimated from experience, however they could end up being parameter if there are many states
    - bellman equation: the value function is essentially: the sum of (probabilities of all outcomes) * (value of outcomes)
        - expresses the value of a state and the value of its successor states
        - starting at a state we have a set of actions, these actions could then create NEW states with new actions, and this could continue happening in successive states
            - the bellman equation basically estimates the value of ALL of these future states and action pairs
    - value of an action is determined based off of the reward of the next action, and the reward of all remaining rewards

3.6) Optimal Policies and Optimal Value Functions)

- a policy is greater than another if the epxected reward is greater in all states
    - there must always exist at least 1 OPTIMAL policy
- bellman optimality equation: the value of a state under the optimal policy must equal the exected reward from the best action from that state
- if you already have the optimal value function, then getting the optimal policy function will be very easy, just need to look ahead one step
    - if you have the optimal value function, then you only need to evaluate short-term values of actions and these actions will be the best long term
    - a "one step ahead search" will yield the optimal actiuons   
- if you know the optimal policy you dont even have to look ahead, the current action values will be optimal
- solving the bellman optimality equation EXPLICITY provides one route to solving the optimal policy, and if you solve the optimal policy then you solve the reinforcement learning problem
    - most times you will actually not be able to solve this equation, as the computational requirement is just too high
- when creating different decision making agents/methods, this can essentially be viewed as approximating the bellman's optimality equation


3.7) optimality and approximation

- an agent that learns an optimal policy has done very well, but the computational cost of this is often unreachable
    - think of it as: a constraint/limit is the computational power an agent can use in a single time step
-  tabular cases: tasks with small finite states, use array or table for each state-action paired
- rl problem forces us to settle for approximating optimal functions, however there may be very many states with low probabilities that approximating suboptimal states/actions might not actually get us anywhere

3.8) Summary

- agent interacts with environment over a discrete amount of time steps
- states are information about the environment, actions are the decisions that the agent makes, rewards are the values of the agent's decisions
- policy: stochastic rule in which the agent selects actions from the enviornment
- value function is the estimated reward from a state-action pair, optimal value function is the largest estimated reward between state-action pairs at every step of the problem

CHAPTER 4) dynammic programming
page 95

- can be used to compute optimal policies GIVEN a PERFECT MODEL of a enviornment
- asume that MDP is finite, and rewards given by a set of probabilities
- going to read the rest of this chapter later, skipping to chapter 5 for now

CHAPTER 5) Monte Carlo Methods:

- first learning methods for estimating value functions and discovering optimal policies
- only require experience, only need streams of states, actions, and reward from actual or simulated actions with the environment
- the model does not need to generate all actions, just needs a sample of actions to begin learning
- method based on averaging sample returns, estimates are only changed at the end of an episode
- returns for EACH state-action pair, stationary problems become non-stationary for each action

5.1)
- begin by learning the state-value function for a given policy 
    - not influencing the actions, just evaluating the current policy for actions
- average returns from state, the more we visit that state the more accurate the return prediction becomes, the estimation will eventually converge to the actual value of the state-action pair
- first visit monte carlo: estimates the value of a state under policy p as the average of the FIRST visit to a state under each episode
    - every visit mc is the same, but we dont JUST use the first occurence of a state in the episode
- blackjack example:
    - can be modeled by monte-carlo
    - the state is based on 2-3 things in the game, 
    - assume some policy where our action is heuristic based on the value of our cards
    - is we continue to play this game many times then eventually we will get the REAL values of the state-value function
        - the state-value function is an approximation but after many games it converges to the real value
- monte carlo is significantly easier to implement then DP approaches, where all of the probabilities of must be computed beforehand
- in monte carlo, the root is the state node, below is the entire trajectory of the state, action, reward pairs that follow in that episode
    - mc diagram only shows the samples in the current episode, but it shows all of them
- estimates for each state are independent
- MC is very useful when a person only needs to know the value of a subset of the states, and doesnt need to know all actual values of all states

5.2) Monte Carlo estimation of action values

- prediction of state-action pairs, and not just state values
- we are going to be estimating the return on a state given an action with policy p
- now we are visiting state-action pairs instead of just states
    - a state-action pair is visited if the same state is followed by the same action before 
- one problem is that not all actions will be explored, which means that we wont get to estimate the value of all state-action pairs
    - in order to truly learn, we need to evaluate the value of ALL state-action pairs, not just the ones that are currently the best to perform
    - to do this we must maintain exploration at all times of simulating episodes, if we do this then all state-action pairs will be sampled an infinite
        amount of times given infinate episodes
- only other way of exploring all state-action pairs is to use a stochastic policy where all actions have a non-zero probability

5.3) Monte Carlo control

- starting to now consider how we can estimate optimal policies
- maintain an approximation of a policy AND a value function
    - changing these two will ultimately make these two better over time
- have a starting policy, play a bunch of episodes, get the value function of actions, update policy greedily (for each state we select the highest valued action)
    - basically, every time we improve the policy we are taking a higher action value for the states, so eventually after improving the policy many times
        we will have the optimal policy, a point where the policy no longer improves because we will be at the optimal action value for each state
    - when we "play a bunch of episodes", we essentially have to gaurentee that every action is tried in every state
        - we do this by maintaining exploration over a large amount of episodes (eventually every action will be selected for every state)
- policy will eventually converge to the optimal policy

- problem: we cant assume infinate episodes for each policy evaluation:
    solution: stick to the sole idea of RL, approximate the values of the policy, dont need to be exact but run a large amount of episodes so that the bounds
        of the approximations are small

- people actually used monte carlo es to SOLVE blackjack

5.4) Monte Carlor Without Exploring Starts

on-policy methods: evaluate the policy that is being used to make the decisions
off policy methods: evaluate a policy that is not being used to generate the data

- on-policy methods start off soft with non-zero probabilities but gradually over time shift to an optimal deterministic policy
    - can use on-policy fist visit monte carlo to change actions and perform better action over time    
        - finally getting to the "learn from experience" section of the RL book 

5.5) Off policy Prediction via Importance Sampling:

- on-policy approach has a flaw: it is learning the policies of NON-OPTIMAL policies, and learning policies that are still exploring
- one approach is to have 2 policies, one policy is solely used to learn and become the optimal policy, the other is solely about exploring
    - target policy is the policy used to learn
    - behaviou policy is the one being used to explore
- on policy methods are simpler, off policy require additional notation and setup
- off-policy learning is seen as very crucial in the world of learning multi-step predicitive models

- behavioral policy must be stochastic and explore different policies
    - ex: e-greedy policy
- target policy can be deterministic

- behavior policy is used for exploring, target policy is used to learn off of the exploring and derive a deteministic optimal policy of the explored
    action-value pairs

- importance sampling makes sure that the target policy does not learn the bad habits of the behavior policy
    - inheritly, the behavior policy will make sub-optimal decisions because it is exploring, but we dont want the target policy to learn these bad decicions,
        only the good ones
- ordinary weighted sampling has the tendency to produce unstable results as the variance is near infinite
- 

5.6) Incremental Implementation:


5.10 -> summary, skipping to this part in book 

- monte carlo essentially defines "actual learning", you play a game a bunch of times, and after you play a bunch of times you are able to 
    derive the actual value of certains decisions and actions BECAUSE you played the game a bunch of times
- monte carlo allows for learning the optimal policies of an environment through experience, which allows the us to have no model that actually knows the 
    dynamics of the enviornment
- advanatges of mc: easy to simulate loads of episodes, easy to evaluate a small subset of states
- we must maintain exploration at all times in monte carlo, we dont want to settle for a sub-optimal policy
- off policy prediction refers to learning a value function of a target policy genereated by a behvaior policy
- page 141 leave off on

Chapter 6) Temperal Difference Learning

- temporal difference learning is a NOVEL idea when it comes to reinforcement learning
    - combines some ideas of dp and monte carlo methods
- both TD and monte carlo use experience to solve the problem, both will update their policies and estimates from the states of the enviornment
- monte carlo methods needs to wait until the end of the episode to update prediction estimates, TD only needs to wait until the next time steps
- TD methods use the current estimate V instead of the true v while also sampling the expected values
- the TD error is the value of the current estimate of S(t) and the updated estimate
    - the monte carlo error can be written as a sum of td errors
        - this identify plays important role in theory and algorithms of TD learning, will prob talk about more later on in the book
- TD learning can essentially take into account factors of a game WHILE the game goes on, think of it as changing your prediction of the result
    while the game is occuring BECUASE OF THE THINGS THAT HAPPEN WHILE PLAYING THE GAME
    - in monte carlo, you need to WAIT until the actual game is over before you can learn anything. But is this necessary? 
        - humans dont need to wait for game to be over to change their predictions

6.2) advantages of td prediction methods:

- td methods learn a guess from a guess, are they better than DP or monte carlo? rest of book talks about this
    - learning every time step and not every episode is actually VERY CRUCIAL in terms of reinforcement learning:
        - sometimes episodes are very long, and sometimes episodes DONT END, there are rl problems where the task is a continuous task
- it has been proven that TD methods ACTUALLY DO converge to the true optimal value function given a policy
- does monte carlo or td converge faster:
    - obv, we want to chose the one that converges faster becomes we will use less compute to solve the rl problem
    - answer is actually no obv and is still unsolved, depends on the fast at hand

6.3) Optimality of TD(0)

- if there is only a finite amount of time steps, then we can go through all time steps then update, we call this batch updating
- batch updating:
    - accumulate a batch/pile/history of episodes
    - use the episodes to change the value function
    - when the value function stops changing is when the value function converges
    - after convergence, we move onto collecting the next batch of episodes


6.4) Sarsa: On-Policy TD control

- monte carlo methods often balance exploitaion and exploration, to do this one approach is to use on-policy vs off-policy, this section will discuss
    on-policy temporal difference
- first step is to learn an action-value function instead of a state-value function
    - if state is terminal, value of action-value is 0
    - otherwise, follows update in book (6.7 page 151)
- Sarsa algorithm: uses s(t), a(t), r(t+1), s(t+1), a(t+1)
- on policy is pretty straitforward, use sarsa to continually estimate q, then use these estimates to update our policy, use these new policy estimates to 
    update the current policy, get better more accurate q's, etc
- sarsa converges with probability 1 to an optimal policy and action-value function as long as all state-action pairs are visited an infinate number of times

leave off on page 152 with the example 6.5 windy gridworld
