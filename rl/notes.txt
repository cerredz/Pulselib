- state, action, rewards, vectors, weights
- rl = learning through interactions
- Capital=random variables, Lowercase=Value of random variable
- 

CHAPTER UNO (1):

- connecting and interacting with your enviornment is a major source of information
    - used int the real world to teach humana things

- we are aware of how our enviornment responds to the things that we do
- rl is focused on "goal-directed" learning from interactions
- want to maximize the reward signal
- learner is not told which actions to take, but rather given the reward
    of the actions and it is up to them to try them out
- trial-and-error search and delayed reward
- rl is NOT supervised learning and NOT unsupervised learning
- agent has to use what it already knows AND explore actions that it has not yet taken
- subproblems of rl enviornments should play clear roles and actually be steps
    towards reaching the central goal of the agent
- modern artificial intelligence is currently looking for methods of 
search, general principles of continuous learning, and decision making
- rl considered many subproblems, we do not tell them explicity what subproblems to 
    try, the agents use their own intuition to explore which subproblems to solve
- rl addresses the scenario of now knowing exactly what to do and 
    real-time action selection
- rl could be a piece of a much larger system, an rl env of
    a body part could feed its result to the rl env of the entire body
- rl has been looks at by many disciplines, not just AI 
- rl most accurately reflects how humans and animals act, and is very apparent in neuroscience and physh
- 

1.2) examples:

- chess moves
- adjust parameters of a operation in real time while taking into account multiple different tradeoffs
- animals in real life
- all the steps in eating breakfast in real life    
    - every decision bounded by a goal
    - each food, must first look for fork, feedback of eyes makes picking up fork possible, then use hand to guide food to eat, done eating, get up, etc 
- rl involves interaction from a decision-making agent, that agents seeks to accomplish a goal despite there being uncertainty in the enviornment
- the effect of actions may not always be able to be 
    fully predicted, so the agent must monitor the response of its env
    - percentage towards goal is determined by what the agent knows in its current state

- interacting  with the enviornment is essentially for IMPROVING the skills of the agent in that task,
    the more they interact with it the better they will become in the future

1.3) elements of rl 

- policy, reward signal, value function, and maybe a model
- policy:
    - defines the way an agent behaves at a given time
    - policies can be stochastic, we can specify probabilities for each action
    - mapping from the enviornment in the current state to the actions it can perform

- reward signal:
    - the goal of the rl problem
    - each action will in return send a reward value to the agent
    - the agent's sole focus is to MAXIMIZE this reward signal over a long period of time
    - 

value function:
    - takes into account the rewards in the LONG TERM
    - takes into account the next states likely to occur next, and calculates the reward for these states combined
    - total amount agent can expect in the future

- immediate reward signal could be low, but the value function could be very high
- rewards are viewed as primary and values are viewed as secondary
    - with no rewards there would be no values
- however, the final decisions that we take are based on the value of value functions
- the most important part of RL is the efficient estimation of values
- model:
    - models are used for planning, we want to predict future states and future rewards before we actually experience them
    - 
- rl env can have both low-level trial and error learning and high-level planning learning

1.4) limitations and scope:

- state: whatever information is available to the agent at a given timeframe
    - what the agent knows at a given time period
- book focused on not creating the state, but what actions the agent should take GIVEN the state
- estimating value functions is NOT required for solving rl problems

1.5) extended example 
- states: table of states, one for each possible state of the game
- value function for state A is "better" than value function for state B if our probability
    of winning in state A is higher
    - if our state has 3 x's, then our prob of winning is 1
    - if state has 3 o's, then prob of winning is 0
    - if neither, prob of winning is .5

- we play many games, most of the time we select states GREEDILY (the states that result in a higher value function)
- occasionally though, we select states "exploratory", the states that dont have a higher value function
    for the sole purpose of exploring different moves
- when we move, we update the previous state's probability to be more accurate to the true probability, since know we can see father ahead
- temporal difference learning: learning is done based on two different states at 2 successive times

- evolutionary methods BAD: if a player wins, all of the states and NON explored states are deemed good as the player won, but this may not be the calculates
- with value function, each state along the way can be used for the agent to learner
    - value function=each state is evaluated
- rl enforces the agent to look ahead and not just determine the immediate rewards of its actions
- reward for rl does not have to the at the end of an episode, it can be before it embeddings
- you can also apply rl to continuous time problems, not just ones with discrete reward signals

- you can setup rl with even infinately many states, the agent will only explore a small
    subset of these states but the final result can still "learn" how to solve the problem the best it can
    - this is a use case for supervised learning combined with rl, need to generalize on 
        our past experiences to make the best possible ones in the future

- rl can be used with or without the use of models to make decisions
- model free systems: enviornments do not have any influent on their actions
- an rl system can be a heirarchical rl system, where each "action" is a seperate 
    rl system


1.6) Summary

- formal framework of rl: states, actions, rewards, 
- value functions for rl are different from evolutionary methods of rl


1.7) History of RL

- one thread of rl was based on trial-and-error in animal learning
- second thread is value function
- "optimal control" threa:
    - first "rl" that solved a problem was known as dynammic programming: the state of the system
        determined the value that it returned
    - solved stochastic optimal control problem
    - the computational requirements grew with the amount of states there where
    - was not obvious how dp can be used to see thinks FORWARD in time since it always relies on 
        information in the past
- optimal control, dp, and rl are very closely intertwined
- "trial and error" thread:
    - "law of effect" -> describes the effect of reinforcing events on the tendency to select actions
    - reinforcement is the strengthening of a behavior based on an animal recieving stimuli
        - or could also be weakening
    - reinforcment produced changes in behavior AFTER the reinforce is removed
    - alan turing "pleasure-pain" system, when pain stimulus occurs all entries are canelled,
        when pleasure stimulus occurs then they are all made permanent
    - first employed and tested with animals
    - rl transitioned in supervised learning, many people thought they were studying rl but were studying   
        nns and supervised learning (generalizable patterns = sl)
    - "klopf" is a g that came along and basically said: hey we should more heavily reply on teh 
        enviornment to learn, rl and supervised learning are different
    - 
- "temperal difference" learning/thread:
    - secondary reinforces are paired with primary reinforces, and whatever behavior the primary
        reinforce triggers is also inherited down to the secondary reinforcer

questions:
    1.3) it would learn to play worst because it never explores other moves. These explored other moves
        that it does not experience could lead to a greater LONG-TERM reward that it will never get to.
    1.4) two sets of probabilities: 
        - we learn the exploratory and non-exploratory set
        - it would be better to learn from exploratory set because we factor in more total
            possible decision paths

    1.5)
        -  play a shit ton of games
        - dont just have all possible states of the tic-tac-toe game, maybe have states whose reward
            is based off of the previous position/s of the game 
        - play against all different types of opponents
        - play multiple games at once, reward is how many games you win in that batch


Chapter 2) multi-arm bandit (page 45)

- rl used the evaluation of actions instead of the instruction of actions to learn
- the feedback of the evaluation is what is necessary for learning in rl, FEEDBACK determines how 
    good the action was
- instruction feedback does not take into account the action itself, just the correct action to take
- evaluation feedback can be combined with instruction feedback

bandit problem:
- repeatedly faced with n different actions, each with a reward, goal is to get the highest reward 
    over certain number of steps
- we do not know the value of the actions beforehand
- select greedily: select action with the highest current estimated reward
- select exploringly: do not select the current highest valued action, with the intuition to explore
    other paths of actions in favor of long-term value benefits
- if you have many steps remaining, it could be better to actu  ally explore the ungreedy actions because 
    we dont know if they will produce a higher value or not and we have many steps to go
- whether it is better to explore or to exploit depends on the estimation of values, uncertainty of 
    those values, and how many steps are left in the rl algorithms
- balancing exploration and exploitation is very big challenge in rl, below are some methods to show
    you the intuitions behind them

action-value methods:
    - each estimate is a simple average of the sample of relevant rewards
    - greedy action: A=argmax(Q(t)(a)) -> a maximizes all q at given step T
    - if we have alot players, and a certain amount of actions, then all actions will theoretically
        be sampled infinately and the estimated value to converge to the ground truth value
    - each action's value is basically the average of the reward of that action experienced
         in the past
left off on page 50: